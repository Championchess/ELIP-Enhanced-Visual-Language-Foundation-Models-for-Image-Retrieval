# ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval

This is the official implementation of the 2025 paper "ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval" by <a href="https://www.robots.ox.ac.uk/~guanqi/" target="_blank">Guanqi Zhan</a>, <a href="https://scholar.google.com/citations?user=GHTB15QAAAAJ&hl=en" target="_blank">Yuanpei Liu</a>, <a href="https://www.kaihan.org" target="_blank">Kai Han</a>, <a href="https://weidixie.github.io/" target="_blank">Weidi Xie</a>, and <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&hl=en" target="_blank">Andrew Zisserman</a>.

The code for CLIP (ELIP-C) and SigLIP (ELIP-S) is based on the OpenCLIP github: https://github.com/mlfoundations/open_clip/tree/main

The code for BLIP-2 (ELIP-B) is based on the LAVIS github: https://github.com/salesforce/LAVIS/tree/main

[The code and dataset in the paper is to be updated]

## Citation
Please cite our papers if you use the code/model/dataset of this github.
```
@article{zhan2025elip,
  title={ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval},
  author={Zhan, Guanqi and Liu, Yuanpei and Han, Kai and Xie, Weidi and Zisserman, Andrew},
  journal={arXiv},
  year={2025}
}
```
